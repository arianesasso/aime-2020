{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import glob\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Custom package\n",
    "import devicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get acc and bvp data for the intervals that blood pressure was measured; exclude exercise period (since no bp measurement was made)\n",
    "def truncate_empatica(data_empatica, bp_df, verbose=False):\n",
    "    sub = pd.DataFrame()\n",
    "    for _, row in bp_df.iterrows():\n",
    "        truncated_df = data_empatica.truncate(before=row['window_start'], after=row['window_end'])\n",
    "        truncated_df['utc'] = row['utc']\n",
    "        truncated_df['bp_sys'] = row['SYS(mmHg)']\n",
    "        truncated_df['bp_dia'] = row['DIA(mmHg)']\n",
    "        truncated_df['subject'] = row['subject']\n",
    "        sub = sub.append(truncated_df)\n",
    "    if verbose:\n",
    "        print(sub.head())\n",
    "        print(sub.describe())\n",
    "        print(sub.shape)\n",
    "    return sub\n",
    "\n",
    "def get_folders(my_path):\n",
    "    folder_names = [f for f in os.listdir(my_path) if os.path.isdir(os.path.join(my_path, f))]\n",
    "    return folder_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ppg_feature_list(data, normalised=False, verbose=False):\n",
    "    list_bp = []\n",
    "    utc = data.groupby(['utc'])\n",
    "    bvp = 'bvp' \n",
    "    if normalised:\n",
    "        bvp += '_normalised'\n",
    "    for _, row in utc:\n",
    "        d = {}\n",
    "        if row['bvp'].any():\n",
    "            d['patientid'] = row['subject'].unique()[0]\n",
    "            d['sbp'] = int(row['bp_sys'].unique()[0])\n",
    "            d['dbp'] = int(row['bp_dia'].unique()[0])\n",
    "            d['ppg'] = row[bvp].tolist()\n",
    "            list_bp.append(d)\n",
    "    return list_bp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motion Sections Removal and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Finds sections with high acceleration magnitude and removes them\n",
    "def remove_motion_sections(df, limit=100, min_size=5, padding=15, std_mult=0.25):\n",
    "    # Todo check frequency domain for high frequencies\n",
    "    acc_mag_mean = df['acc_mag'].mean()\n",
    "    acc_mag_std =  df['acc_mag'].std()\n",
    "    # Comparison with overall mean and std\n",
    "    thresh_indices = np.squeeze(np.argwhere((df['acc_mag'].values > acc_mag_mean + std_mult * acc_mag_std) | \n",
    "                                            (df['acc_mag'].values < acc_mag_mean - std_mult * acc_mag_std)))\n",
    "        \n",
    "    section_indices = []\n",
    "    section_start = thresh_indices[0]\n",
    "    for i in range(1, len(thresh_indices) - 1):\n",
    "        if thresh_indices[i] - thresh_indices[i-1] > limit:\n",
    "            if thresh_indices[i-1] >= section_start + min_size:\n",
    "                section_indices.append((section_start - padding, thresh_indices[i-1] + padding))\n",
    "            section_start = thresh_indices[i]\n",
    "    if thresh_indices[-1] != section_start:\n",
    "        section_indices.append((section_start, thresh_indices[-1]))\n",
    "    section_indices.reverse()\n",
    "    for (start, end) in section_indices:\n",
    "        df = df.drop(index=df.iloc[start:end].index)\n",
    "    return df\n",
    "\n",
    "def apply_filters(df):\n",
    "    # No smoothing neccessary due to relatively low sampling frequency\n",
    "    df['bvp_normalised'] = (df['bvp'] - df['bvp'].min()) / (df['bvp'].max() - df['bvp'].min())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('../../config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "exp_base_path = config['hype']\n",
    "print(exp_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Json for One Patient - Biking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp_date = 'yyyy-mm-dd'\n",
    "exp_patient = 'patient_id'\n",
    "experiment = 'biking'\n",
    "filtering = 'bfill'\n",
    "\n",
    "patient_base_path = os.path.join(exp_base_path, exp_date, exp_patient)\n",
    "print(patient_base_path)\n",
    "    \n",
    "# Sources\n",
    "sources = {\n",
    "            'tag' : glob.glob(patient_base_path+r'/Tag*').pop(),\n",
    "            'faros' : glob.glob(patient_base_path+r'/Faros*').pop(),\n",
    "            'empatica' : glob.glob(patient_base_path+r'/Empatica*').pop(),\n",
    "            'spacelabs' : glob.glob(patient_base_path+r'/*SpaceLabs*').pop(),\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "empatica = devicely.EmpaticaReader(sources['empatica'])\n",
    "empatica.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists(sources['spacelabs']):\n",
    "    for file in os.listdir(sources['spacelabs']):\n",
    "        if file.endswith(\".abp\"):\n",
    "            spacelabsfile = os.path.join(sources['spacelabs'], file)\n",
    "            break\n",
    "print(spacelabsfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bp = devicely.SpacelabsReader(spacelabsfile, 2)\n",
    "bp.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time_delta = '30 seconds'\n",
    "time_delta_modified = time_delta.split(' ')\n",
    "time_delta_dict = {time_delta_modified[1]: int(time_delta_modified[0])}\n",
    "\n",
    "bp.set_window(datetime.timedelta(**time_delta_dict), filtering)\n",
    "bp.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bp_df = bp.data.drop(['error','z'], axis=1).reset_index().copy()\n",
    "bp_df.rename(columns={\"datetime\": \"utc\"}, inplace=True)\n",
    "bp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subset_empatica = empatica.data[['bvp','acc_mag']].dropna(how='all')\n",
    "sub_data_empatica = truncate_empatica(subset_empatica, bp_df, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data_empatica.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional filtering\n",
    "sub_data_empatica_motionless = remove_motion_sections(sub_data_empatica)\n",
    "sub_data_empatica_filtered = apply_filters(sub_data_empatica_motionless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = ppg_feature_list(sub_data_empatica_filtered, normalised=True, verbose=False)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create features path if it not exists\n",
    "features_path = os.path.join('../../features/hype-json', today, experiment, filtering, time_delta.replace(' ',''))\n",
    "if not os.path.exists(features_path):\n",
    "    os.makedirs(features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(features_path,str(bp_df['subject'].unique()[0])+'_ppg_feature_list_'+time_delta.replace(' ','')+'.json'), 'w') as f:\n",
    "    json.dump(features, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Json for All Patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set Blood Pressure Monitor and Window\n",
    "bp_monitor = 'spacelabs'\n",
    "time_delta = '30 seconds'\n",
    "timeshift = 2 # for converting bp_time to utc\n",
    "today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "verbose = False\n",
    "normalised = True\n",
    "motionless = True\n",
    "experiment = 'biking'\n",
    "filtering = 'bfill'\n",
    "\n",
    "dates = get_folders(exp_base_path)\n",
    "all_features = []\n",
    "\n",
    "for date in dates:\n",
    "    print(date)\n",
    "    subjects = get_folders(os.path.join(exp_base_path, date))\n",
    "    for subject in subjects:\n",
    "        print(subject)\n",
    "        patient_base_path = os.path.join(exp_base_path, date, subject)\n",
    "        tag = os.path.join(patient_base_path, 'Tags')\n",
    "        \n",
    "        # Check for the Tags\n",
    "        if not os.path.exists(tag):\n",
    "            print(\"No Tag File.\")\n",
    "            print('-----','\\n')\n",
    "            break\n",
    "        \n",
    "        # Sources\n",
    "        sources = {\n",
    "                    'tag' : glob.glob(patient_base_path+r'/Tag*').pop(),\n",
    "                    'faros' : glob.glob(patient_base_path+r'/Faros*').pop(),\n",
    "                    'empatica' : glob.glob(patient_base_path+r'/Empatica*').pop()\n",
    "                  }\n",
    "    \n",
    "        if bp_monitor == 'spacelabs':\n",
    "            sources['spacelabs'] = glob.glob(patient_base_path+r'/*SpaceLabs*').pop()\n",
    "            \n",
    "            if not os.path.exists(sources['spacelabs']):\n",
    "                print(\"Subject has no spacelabs file\")\n",
    "                break\n",
    "             # Read Spacelabs\n",
    "            for file in os.listdir(sources['spacelabs']):\n",
    "                if file.endswith(\".abp\"):\n",
    "                    spacelabs_file = os.path.join(sources['spacelabs'], file)\n",
    "                    break\n",
    "            \n",
    "            bp = devicely.SpacelabsReader(spacelabs_file, timeshift)\n",
    "            \n",
    "            time_delta_modified = time_delta.split(' ')\n",
    "            time_delta_dict = {time_delta_modified[1]: int(time_delta_modified[0])}\n",
    "            bp.set_window(datetime.timedelta(**time_delta_dict), filtering)\n",
    "        \n",
    "            # Adjust columns\n",
    "            bp_df = bp.data.drop(['error','z'], axis=1).reset_index().copy()\n",
    "            bp_df.rename(columns={\"datetime\": \"utc\"}, inplace=True)\n",
    "        if verbose: print(bp_df.head(1))\n",
    "            \n",
    "        # Read Empatica\n",
    "        empatica = devicely.EmpaticaReader(sources['empatica'])\n",
    "        subset_empatica = empatica.data[['bvp','acc_mag']].dropna(how='all').copy()\n",
    "        if verbose: print(subset_empatica.head(1))\n",
    "    \n",
    "        # Extract Features\n",
    "        data_empatica = truncate_empatica(subset_empatica, bp_df, verbose=verbose)\n",
    "        print(\"Truncated data shape: \", data_empatica.shape)\n",
    "        # Remove Motion and Apply filters\n",
    "        if motionless:\n",
    "            data_empatica = remove_motion_sections(data_empatica)\n",
    "            print(\"Montionless data shape: \", data_empatica.shape)\n",
    "        if normalised:\n",
    "            data_empatica = apply_filters(data_empatica)\n",
    "            print(\"Normalised bvp mean: \", data_empatica['bvp_normalised'].mean())\n",
    "        # Get PPG values for each bp pair           \n",
    "        features = ppg_feature_list(data_empatica, normalised=normalised, verbose=verbose)\n",
    "\n",
    "        if not features:\n",
    "            print('Subject has no features.')\n",
    "            print('-----','\\n')\n",
    "            break\n",
    "        else:\n",
    "            print(\"Number of features:\", len(features))\n",
    "\n",
    "        # Create features path if it not exists\n",
    "        features_path = os.path.join('../../features/hype-json', today, experiment, filtering, time_delta.replace(' ',''))\n",
    "        if normalised:\n",
    "            directory = 'normalised'\n",
    "            if motionless:\n",
    "                directory = 'normalised-motionless'\n",
    "            else:\n",
    "                directory = 'normalised-motion'\n",
    "        elif not normalised and motionless:\n",
    "            directory = 'not-normalised-motionless'\n",
    "        else:\n",
    "            directory = 'raw'\n",
    "        features_path = os.path.join(features_path, directory)\n",
    "        \n",
    "        if not os.path.exists(features_path):\n",
    "            os.makedirs(features_path)\n",
    "            \n",
    "        if all_features:\n",
    "            all_features = all_features + features\n",
    "        else:\n",
    "            all_features = features\n",
    "    \n",
    "        with open(os.path.join(features_path, str(bp_df['subject'].unique()[0])+'_ppg_feature_list_'+time_delta.replace(' ','')+'.json'), 'w') as f:\n",
    "            json.dump(features, f)\n",
    "        if verbose: \n",
    "            print(features)\n",
    "            \n",
    "        print('-----','\\n')\n",
    "\n",
    "print(\"Total of pairs bp/ppg: \", len(all_features))\n",
    "with open(os.path.join(features_path,'all_features_'+time_delta.replace(' ','')+'.json'), 'w') as f:\n",
    "    json.dump(all_features, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24 Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Blood Pressure Monitor and Window\n",
    "bp_monitor = 'spacelabs'\n",
    "time_delta = '30 seconds'\n",
    "timeshift = 2 # for converting bp_time to utc\n",
    "today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "verbose = False\n",
    "normalised = True\n",
    "motionless = True\n",
    "experiment = '24hours'\n",
    "filtering = 'bfill'\n",
    "\n",
    "dates = get_folders(exp_base_path)\n",
    "all_features = []\n",
    "\n",
    "for date in dates:\n",
    "    print(date)\n",
    "    subjects = get_folders(os.path.join(exp_base_path, date))\n",
    "    for subject in subjects:\n",
    "        print(subject)\n",
    "        patient_base_path = os.path.join(exp_base_path, date, subject, '24 hours')\n",
    "        \n",
    "        # Sources\n",
    "        sources = {\n",
    "                    'faros' : glob.glob(patient_base_path+r'/Faros*').pop(),\n",
    "                    'empatica' : glob.glob(patient_base_path+r'/Empatica*').pop()\n",
    "                  }\n",
    "    \n",
    "        if bp_monitor == 'spacelabs':\n",
    "            sources['spacelabs'] = glob.glob(patient_base_path+r'/*SpaceLabs*').pop()\n",
    "            \n",
    "            if not os.path.exists(sources['spacelabs']):\n",
    "                print(\"Subject has no spacelabs file\")\n",
    "                break\n",
    "             # Read Spacelabs\n",
    "            for file in os.listdir(sources['spacelabs']):\n",
    "                if file.endswith(\".abp\"):\n",
    "                    spacelabs_file = os.path.join(sources['spacelabs'], file)\n",
    "                    break\n",
    "            \n",
    "            bp = devicely.SpacelabsReader(spacelabs_file, timeshift)\n",
    "            \n",
    "            time_delta_modified = time_delta.split(' ')\n",
    "            time_delta_dict = {time_delta_modified[1]: int(time_delta_modified[0])}\n",
    "            bp.set_window(datetime.timedelta(**time_delta_dict), filtering)\n",
    "        \n",
    "            # Adjust columns\n",
    "            bp_df = bp.data.drop(['error','z'], axis=1).reset_index().copy()\n",
    "            bp_df.rename(columns={\"datetime\": \"utc\"}, inplace=True)                                          \n",
    "        if verbose: print(bp_df.head(1))\n",
    "            \n",
    "        # Read Empatica\n",
    "        empatica = devicely.EmpaticaReader(sources['empatica'])\n",
    "        subset_empatica = empatica.data[['bvp','acc_mag']].dropna(how='all').copy()\n",
    "        if verbose: print(subset_empatica.head(1))\n",
    "    \n",
    "        # Extract Features\n",
    "        data_empatica = truncate_empatica(subset_empatica, bp_df, verbose=verbose)\n",
    "        print(\"Truncated data shape: \", data_empatica.shape)\n",
    "        # Remove Motion and Apply filters\n",
    "        if motionless:\n",
    "            data_empatica = remove_motion_sections(data_empatica)\n",
    "            print(\"Montionless data shape: \", data_empatica.shape)\n",
    "        if normalised:\n",
    "            data_empatica = apply_filters(data_empatica)\n",
    "            print(\"Normalised bvp mean: \", data_empatica['bvp_normalised'].mean())\n",
    "        # Get PPG values for each bp pair           \n",
    "        features = ppg_feature_list(data_empatica, normalised=normalised, verbose=verbose)\n",
    "\n",
    "        if not features:\n",
    "            print('Subject has no features.')\n",
    "            print('-----','\\n')\n",
    "            break\n",
    "        else:\n",
    "            print(\"Number of features:\", len(features))\n",
    "\n",
    "        # Create features path if it not exists\n",
    "        features_path = os.path.join('../../features/hype-json', today, experiment, filtering, time_delta.replace(' ',''))\n",
    "        if normalised:\n",
    "            directory = 'normalised'\n",
    "            if motionless:\n",
    "                directory = 'normalised-motionless'\n",
    "            else:\n",
    "                directory = 'normalised-motion'\n",
    "        elif not normalised and motionless:\n",
    "            directory = 'not-normalised-motionless'\n",
    "        else:\n",
    "            directory = 'raw'\n",
    "        features_path = os.path.join(features_path, directory)\n",
    "\n",
    "        if not os.path.exists(features_path):\n",
    "            os.makedirs(features_path)\n",
    "            \n",
    "        if all_features:\n",
    "            all_features = all_features + features\n",
    "        else:\n",
    "            all_features = features\n",
    "    \n",
    "        with open(os.path.join(features_path, str(bp_df['subject'].unique()[0])+'_ppg_feature_list_'+time_delta.replace(' ','')+'.json'), 'w') as f:\n",
    "            json.dump(features, f)\n",
    "        if verbose: \n",
    "            print(features)\n",
    "            \n",
    "        print('-----','\\n')\n",
    "\n",
    "print(\"Total of pairs bp/ppg: \", len(all_features))\n",
    "with open(os.path.join(features_path,'all_features_'+time_delta.replace(' ','')+'.json'), 'w') as f:\n",
    "    json.dump(all_features, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
